{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('FrozenLake-v1')\n",
    "for state in env.P:\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for a in env.P[state]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Iteration:\n",
    "    def __init__(self,gamma,env,theta):\n",
    "        self.gamma=gamma\n",
    "        self.env=env\n",
    "        self.V=np.zeros(self.env.observation_space.n)\n",
    "        self.theta=theta  \n",
    "        \n",
    "        self.pi =(1/env.action_space.n)*np.ones((self.env.observation_space.n,self.env.action_space.n))\n",
    "    def policy_iteration(self):\n",
    "        \n",
    "        delta=0\n",
    "        d=np.zeros((self.env.observation_space.n,self.env.action_space.n))\n",
    "       \n",
    "        while delta<self.theta:\n",
    "            a=np.zeros(self.env.observation_space.n)\n",
    "            for state in self.env.P:\n",
    "                v=self.V[state].copy()\n",
    "                for action in self.env.P[state]:\n",
    "                    \n",
    "                    for prob,next_state,reward,terminated in env.P[state][action]:\n",
    "                        d[state][action]+=prob*(reward+self.gamma*self.V[next_state])\n",
    "                    a[state]+=self.pi[state][action]*d[state][action]\n",
    "                self.V[state]=a[state].copy()\n",
    "                  \n",
    "                delta=max(delta,abs(self.V[state]-v))\n",
    "                   \n",
    "    def policy_improvement(self):\n",
    "        \n",
    "        policy_stable=True\n",
    "        q=np.zeros((self.env.observation_space.n,self.env.action_space.n))\n",
    "        for state in self.env.P:\n",
    "            old_pi=self.pi[state].copy()   \n",
    "            for action in self.env.P[state]:\n",
    "                for prob,next_state,reward,terminated in self.env.P[state][action]:\n",
    "                    q[state][action]+=prob*(reward+self.gamma*self.V[next_state])\n",
    "            best_actions=np.where(q[state]==np.max(q[state]))\n",
    "            self.pi[state]=np.zeros(self.env.action_space.n)\n",
    "            self.pi[state , best_actions]=1/np.size(best_actions)\n",
    "            print('old_pi: ',old_pi)\n",
    "            print('pi :',self.pi[state])\n",
    "            \n",
    "            if(np.allclose(self.pi[state],old_pi)):\n",
    "                policy_stable=False\n",
    "       \n",
    "        if not policy_stable:\n",
    "            self.policy_iteration()\n",
    "    def test(self,eps,env):\n",
    "        for i in range(eps):\n",
    "            done=False\n",
    "            state,_=env.reset()\n",
    "            action=np.random.choice(np.where(self.pi[state]==np.max(self.pi[state])[0]))\n",
    "            env.render()\n",
    "            while not done:\n",
    "                new_state,reward,terminated,_,_=env.step(action)\n",
    "                action=np.random.choice(np.where(self.pi[new_state]==np.max(self.pi[new_state]))[0])\n",
    "                state=new_state\n",
    "                done=terminated\n",
    "        env.close()\n",
    "    \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "                    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.25 0.  ]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0. 1. 0. 0.]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0. 0. 1. 0.]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0. 0. 1. 0.]\n",
      "old_pi:  [0.25 0.25 0.25 0.25]\n",
      "pi : [0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "model=Policy_Iteration(0.9,env,0.000000001)\n",
    "model.policy_iteration()\n",
    "print(model.V)\n",
    "model.policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False,render_mode='human')\n",
    "# model.test(5,env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions=np.where(model.q[10]==np.max(model.q[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
